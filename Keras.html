
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>TensorFlow: Keras &#8212; Introduction to Python</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Introduction to Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Python_introduction.html">
   First steps
  </a>
 </li>
</ul>
    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Keras.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FKeras.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/Keras.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#a-more-complex-model">
   A more complex model
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="tensorflow-keras">
<h1>TensorFlow: Keras<a class="headerlink" href="#tensorflow-keras" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://www.tensorflow.org">TensorFlow</a> is a machine learning and artifical intelligence platform. While including broad functionality, we will focus here on artificial neural networks (ANN). To build such networks, we will leverage the high level API <a class="reference external" href="https://keras.io/">Keras</a> and show here how to use ‘the sequential model’. This is allows to create the simplest form of neural networks, a fully connected feed forward structure. This unhandy term basically refers to a stack of so called layers of neurons, where one neuron of one layer is ‘connected’ to each neuron of the subsequent layer (as we won’t go into theoretic details: <a class="reference external" href="https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf">original paper</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Feedforward_neural_network">Wikipedia</a>, <a class="reference external" href="https://web.stanford.edu/~hastie/Papers/ESLII.pdf">The Elements of Statistical Learning, chapter 11</a>).</p>
<p>TensorFlow works with numpy arrays and pandas dataframes. We will now try to do a regression on some simulated data, using a neural network.</p>
<p>The package <code class="docutils literal notranslate"><span class="pre">tensorflow</span></code> must be installed.
To build neural networks, we start by importing the necessary modules. Note that Keras is delivered as a submodule to TensorFlow, which we can access using the dot chaining, and that we only import <code class="docutils literal notranslate"><span class="pre">layers</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span><span class="p">,</span> <span class="n">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">102</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We will write a function to simulate some data using numpy. We include a <code class="docutils literal notranslate"><span class="pre">noise</span></code> argument to be able to view the true function, which is obfuscated by the added noise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">400</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">make_data</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">_x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">noise</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">*</span><span class="n">_x</span> <span class="o">+</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">_x</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">c</span><span class="o">*</span><span class="n">_x</span><span class="o">**</span><span class="mi">3</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">40</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">_x</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">noise</span><span class="o">==</span><span class="kc">False</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">a</span><span class="o">*</span><span class="n">_x</span> <span class="o">+</span> <span class="n">b</span><span class="o">*</span><span class="p">(</span><span class="n">_x</span><span class="o">-</span><span class="mi">4</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">c</span><span class="o">*</span><span class="n">_x</span><span class="o">**</span><span class="mi">3</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">make_data</span><span class="p">(</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">make_data</span><span class="p">(</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Keras_4_0.png" src="_images/Keras_4_0.png" />
</div>
</div>
<p>We define a very simple fully connected ANN with one hidden layer and only zwo neurons inside that layer. Since we aim to perform regression, we use one neuron in the output layer.
To do so in the code, we kall <code class="docutils literal notranslate"><span class="pre">keras.Sequential</span></code> and add as argument all layers as a list. Elements of that list are <code class="docutils literal notranslate"><span class="pre">keras.layers.Dense</span></code> objects, which represent fully connected layers. Note that the order by which you define this list, corresponds to the stacking of the layers.
As arguments of <code class="docutils literal notranslate"><span class="pre">Dense</span></code>, we specify the number of neurons and the desired activation function. We may also give names to the layers, which might be ore useful for more complex net architectures.</p>
<p>Be aware that we do define an input shape here. Instead, we could let Keras do so automatically using the dimension of our input data. However, the specification of the input shape enables us to view the model summary without having to run anything else beforehand.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;hidden_layer&quot;</span><span class="p">),</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;output_layer&quot;</span><span class="p">),</span>
<span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;my_first_model&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2021-10-14 10:45:23.575602: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
</pre></div>
</div>
</div>
</div>
<p>A summary of the definded model can be printed calling the method of the same name.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;my_first_model&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
hidden_layer (Dense)         (None, 2)                 4         
_________________________________________________________________
output_layer (Dense)         (None, 1)                 3         
=================================================================
Total params: 7
Trainable params: 7
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
<p>Having defined the model, we must compile, i.e. set the final configuration. We call the <code class="docutils literal notranslate"><span class="pre">.compile()</span></code> method on our model and specify the optimization algorithm (and the learning rate) and the loss function, by which the model should be trained. For regression, we use the mean squared error.</p>
<p>The learning rate is a hyperparameter and is to be chosen to suit the problem at hand. It’s possible to find an optimal rate by random or grid search, which we will not further discuss here.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">5e-2</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">MeanSquaredError</span><span class="p">(),</span> <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can now train the model, i.e. set the weights accordingly as to minimize the calculated error from deviations between model predictions and training data.</p>
<p>We do so by calling the <code class="docutils literal notranslate"><span class="pre">.fit()</span></code> method to which we supply the data and the number of episodes (iterations to tune the weights). Furthermore, we can set the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code>, which defines the number of samples for which the weight updates are accumulated.</p>
<p>We can save all information returned from the fitting process in a <code class="docutils literal notranslate"><span class="pre">History</span></code> object, here called <code class="docutils literal notranslate"><span class="pre">history</span></code>. Through this object, we can access this information, once the fitting is done.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">30</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2021-10-14 10:45:23.809495: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/150
14/14 [==============================] - 0s 657us/step - loss: 4470.3457
Epoch 2/150
14/14 [==============================] - 0s 619us/step - loss: 3971.0054
Epoch 3/150
14/14 [==============================] - 0s 621us/step - loss: 3467.3037
Epoch 4/150
14/14 [==============================] - 0s 631us/step - loss: 3079.7207
Epoch 5/150
14/14 [==============================] - 0s 638us/step - loss: 2916.2766
Epoch 6/150
14/14 [==============================] - 0s 619us/step - loss: 2892.6719
Epoch 7/150
14/14 [==============================] - 0s 617us/step - loss: 2861.6719
Epoch 8/150
14/14 [==============================] - 0s 607us/step - loss: 2846.9226
Epoch 9/150
14/14 [==============================] - 0s 566us/step - loss: 2818.4285
Epoch 10/150
14/14 [==============================] - 0s 562us/step - loss: 2793.8281
Epoch 11/150
14/14 [==============================] - 0s 525us/step - loss: 2761.9172
Epoch 12/150
14/14 [==============================] - 0s 563us/step - loss: 2735.2073
Epoch 13/150
14/14 [==============================] - 0s 541us/step - loss: 2712.9531
Epoch 14/150
14/14 [==============================] - 0s 581us/step - loss: 2679.9016
Epoch 15/150
14/14 [==============================] - 0s 614us/step - loss: 2652.0378
Epoch 16/150
14/14 [==============================] - 0s 595us/step - loss: 2624.5657
Epoch 17/150
14/14 [==============================] - 0s 539us/step - loss: 2603.8875
Epoch 18/150
14/14 [==============================] - 0s 563us/step - loss: 2568.0142
Epoch 19/150
14/14 [==============================] - 0s 560us/step - loss: 2534.2151
Epoch 20/150
14/14 [==============================] - 0s 596us/step - loss: 2501.3274
Epoch 21/150
14/14 [==============================] - 0s 578us/step - loss: 2469.9299
Epoch 22/150
14/14 [==============================] - 0s 565us/step - loss: 2437.1091
Epoch 23/150
14/14 [==============================] - 0s 549us/step - loss: 2404.6475
Epoch 24/150
14/14 [==============================] - 0s 603us/step - loss: 2389.3669
Epoch 25/150
14/14 [==============================] - 0s 563us/step - loss: 2349.2217
Epoch 26/150
14/14 [==============================] - 0s 550us/step - loss: 2315.2461
Epoch 27/150
14/14 [==============================] - 0s 558us/step - loss: 2293.5442
Epoch 28/150
14/14 [==============================] - 0s 551us/step - loss: 2267.4780
Epoch 29/150
14/14 [==============================] - 0s 551us/step - loss: 2231.2209
Epoch 30/150
14/14 [==============================] - 0s 591us/step - loss: 2202.1086
Epoch 31/150
14/14 [==============================] - 0s 553us/step - loss: 2186.7854
Epoch 32/150
14/14 [==============================] - 0s 546us/step - loss: 2156.5813
Epoch 33/150
14/14 [==============================] - 0s 602us/step - loss: 2146.9822
Epoch 34/150
14/14 [==============================] - 0s 548us/step - loss: 2118.5540
Epoch 35/150
14/14 [==============================] - 0s 540us/step - loss: 2101.2888
Epoch 36/150
14/14 [==============================] - 0s 562us/step - loss: 2083.7998
Epoch 37/150
14/14 [==============================] - 0s 576us/step - loss: 2061.8826
Epoch 38/150
14/14 [==============================] - 0s 529us/step - loss: 2047.3331
Epoch 39/150
14/14 [==============================] - 0s 638us/step - loss: 2031.5485
Epoch 40/150
14/14 [==============================] - 0s 607us/step - loss: 2017.3744
Epoch 41/150
14/14 [==============================] - 0s 627us/step - loss: 1997.7727
Epoch 42/150
14/14 [==============================] - 0s 613us/step - loss: 1976.8397
Epoch 43/150
14/14 [==============================] - 0s 566us/step - loss: 1954.6996
Epoch 44/150
14/14 [==============================] - 0s 572us/step - loss: 1951.4686
Epoch 45/150
14/14 [==============================] - 0s 591us/step - loss: 1945.6318
Epoch 46/150
14/14 [==============================] - 0s 604us/step - loss: 1931.9739
Epoch 47/150
14/14 [==============================] - 0s 563us/step - loss: 1917.6132
Epoch 48/150
14/14 [==============================] - 0s 540us/step - loss: 1903.6715
Epoch 49/150
14/14 [==============================] - 0s 561us/step - loss: 1896.2167
Epoch 50/150
14/14 [==============================] - 0s 553us/step - loss: 1891.5952
Epoch 51/150
14/14 [==============================] - 0s 588us/step - loss: 1892.2567
Epoch 52/150
14/14 [==============================] - 0s 567us/step - loss: 1877.8528
Epoch 53/150
14/14 [==============================] - 0s 536us/step - loss: 1879.6250
Epoch 54/150
14/14 [==============================] - 0s 539us/step - loss: 1868.3823
Epoch 55/150
14/14 [==============================] - 0s 553us/step - loss: 1865.5453
Epoch 56/150
14/14 [==============================] - 0s 553us/step - loss: 1860.5211
Epoch 57/150
14/14 [==============================] - 0s 564us/step - loss: 1854.0654
Epoch 58/150
14/14 [==============================] - 0s 585us/step - loss: 1860.5696
Epoch 59/150
14/14 [==============================] - 0s 567us/step - loss: 1849.3143
Epoch 60/150
14/14 [==============================] - 0s 577us/step - loss: 1847.8187
Epoch 61/150
14/14 [==============================] - 0s 547us/step - loss: 1850.0186
Epoch 62/150
14/14 [==============================] - 0s 582us/step - loss: 1834.4703
Epoch 63/150
14/14 [==============================] - 0s 561us/step - loss: 1854.7517
Epoch 64/150
14/14 [==============================] - 0s 586us/step - loss: 1824.2429
Epoch 65/150
14/14 [==============================] - 0s 543us/step - loss: 1843.6569
Epoch 66/150
14/14 [==============================] - 0s 562us/step - loss: 1841.7762
Epoch 67/150
14/14 [==============================] - 0s 541us/step - loss: 1840.6553
Epoch 68/150
14/14 [==============================] - 0s 530us/step - loss: 1830.9075
Epoch 69/150
14/14 [==============================] - 0s 612us/step - loss: 1831.5658
Epoch 70/150
14/14 [==============================] - 0s 597us/step - loss: 1834.8336
Epoch 71/150
14/14 [==============================] - 0s 609us/step - loss: 1827.5754
Epoch 72/150
14/14 [==============================] - 0s 579us/step - loss: 1847.2633
Epoch 73/150
14/14 [==============================] - 0s 561us/step - loss: 1835.8331
Epoch 74/150
14/14 [==============================] - 0s 570us/step - loss: 1832.0385
Epoch 75/150
14/14 [==============================] - 0s 557us/step - loss: 1823.3914
Epoch 76/150
14/14 [==============================] - 0s 569us/step - loss: 1835.5992
Epoch 77/150
14/14 [==============================] - 0s 545us/step - loss: 1827.6813
Epoch 78/150
14/14 [==============================] - 0s 538us/step - loss: 1826.5475
Epoch 79/150
14/14 [==============================] - 0s 549us/step - loss: 1830.8411
Epoch 80/150
14/14 [==============================] - 0s 565us/step - loss: 1831.2275
Epoch 81/150
14/14 [==============================] - 0s 546us/step - loss: 1825.0341
Epoch 82/150
14/14 [==============================] - 0s 533us/step - loss: 1830.7521
Epoch 83/150
14/14 [==============================] - 0s 578us/step - loss: 1834.6223
Epoch 84/150
14/14 [==============================] - 0s 576us/step - loss: 1827.4984
Epoch 85/150
14/14 [==============================] - 0s 560us/step - loss: 1832.7499
Epoch 86/150
14/14 [==============================] - 0s 549us/step - loss: 1834.8938
Epoch 87/150
14/14 [==============================] - 0s 594us/step - loss: 1834.5044
Epoch 88/150
14/14 [==============================] - 0s 577us/step - loss: 1831.0520
Epoch 89/150
14/14 [==============================] - 0s 590us/step - loss: 1825.5806
Epoch 90/150
14/14 [==============================] - 0s 565us/step - loss: 1833.1958
Epoch 91/150
14/14 [==============================] - 0s 529us/step - loss: 1832.6318
Epoch 92/150
14/14 [==============================] - 0s 568us/step - loss: 1819.5370
Epoch 93/150
14/14 [==============================] - 0s 551us/step - loss: 1845.5442
Epoch 94/150
14/14 [==============================] - 0s 593us/step - loss: 1823.3862
Epoch 95/150
14/14 [==============================] - 0s 589us/step - loss: 1824.8311
Epoch 96/150
14/14 [==============================] - 0s 589us/step - loss: 1835.6841
Epoch 97/150
14/14 [==============================] - 0s 593us/step - loss: 1822.9773
Epoch 98/150
14/14 [==============================] - 0s 601us/step - loss: 1826.6064
Epoch 99/150
14/14 [==============================] - 0s 578us/step - loss: 1825.3503
Epoch 100/150
14/14 [==============================] - 0s 597us/step - loss: 1823.7723
Epoch 101/150
14/14 [==============================] - 0s 580us/step - loss: 1834.3936
Epoch 102/150
14/14 [==============================] - 0s 553us/step - loss: 1831.4547
Epoch 103/150
14/14 [==============================] - 0s 536us/step - loss: 1833.5236
Epoch 104/150
14/14 [==============================] - 0s 556us/step - loss: 1832.1366
Epoch 105/150
14/14 [==============================] - 0s 537us/step - loss: 1827.3495
Epoch 106/150
14/14 [==============================] - 0s 554us/step - loss: 1829.1506
Epoch 107/150
14/14 [==============================] - 0s 547us/step - loss: 1836.5776
Epoch 108/150
14/14 [==============================] - 0s 548us/step - loss: 1829.8143
Epoch 109/150
14/14 [==============================] - 0s 566us/step - loss: 1828.6045
Epoch 110/150
14/14 [==============================] - 0s 548us/step - loss: 1822.3267
Epoch 111/150
14/14 [==============================] - 0s 586us/step - loss: 1824.6053
Epoch 112/150
14/14 [==============================] - 0s 570us/step - loss: 1828.5239
Epoch 113/150
14/14 [==============================] - 0s 530us/step - loss: 1826.0149
Epoch 114/150
14/14 [==============================] - 0s 551us/step - loss: 1829.6566
Epoch 115/150
14/14 [==============================] - 0s 514us/step - loss: 1834.6272
Epoch 116/150
14/14 [==============================] - 0s 556us/step - loss: 1825.8113
Epoch 117/150
14/14 [==============================] - 0s 531us/step - loss: 1832.1154
Epoch 118/150
14/14 [==============================] - 0s 539us/step - loss: 1830.7510
Epoch 119/150
14/14 [==============================] - 0s 543us/step - loss: 1834.1184
Epoch 120/150
14/14 [==============================] - 0s 580us/step - loss: 1822.8207
Epoch 121/150
14/14 [==============================] - 0s 554us/step - loss: 1822.2584
Epoch 122/150
14/14 [==============================] - 0s 566us/step - loss: 1829.0576
Epoch 123/150
14/14 [==============================] - 0s 552us/step - loss: 1833.0251
Epoch 124/150
14/14 [==============================] - 0s 554us/step - loss: 1830.8590
Epoch 125/150
14/14 [==============================] - 0s 555us/step - loss: 1841.4994
Epoch 126/150
14/14 [==============================] - 0s 568us/step - loss: 1825.7443
Epoch 127/150
14/14 [==============================] - 0s 524us/step - loss: 1823.9399
Epoch 128/150
14/14 [==============================] - 0s 554us/step - loss: 1830.7111
Epoch 129/150
14/14 [==============================] - 0s 560us/step - loss: 1818.4619
Epoch 130/150
14/14 [==============================] - 0s 555us/step - loss: 1837.2064
Epoch 131/150
14/14 [==============================] - 0s 573us/step - loss: 1828.2599
Epoch 132/150
14/14 [==============================] - 0s 546us/step - loss: 1826.8867
Epoch 133/150
14/14 [==============================] - 0s 532us/step - loss: 1827.8370
Epoch 134/150
14/14 [==============================] - 0s 533us/step - loss: 1827.1299
Epoch 135/150
14/14 [==============================] - 0s 560us/step - loss: 1830.8934
Epoch 136/150
14/14 [==============================] - 0s 551us/step - loss: 1835.8800
Epoch 137/150
14/14 [==============================] - 0s 553us/step - loss: 1822.5099
Epoch 138/150
14/14 [==============================] - 0s 577us/step - loss: 1826.5615
Epoch 139/150
14/14 [==============================] - 0s 580us/step - loss: 1836.3561
Epoch 140/150
14/14 [==============================] - 0s 571us/step - loss: 1820.7600
Epoch 141/150
14/14 [==============================] - 0s 567us/step - loss: 1820.0609
Epoch 142/150
14/14 [==============================] - 0s 543us/step - loss: 1835.3673
Epoch 143/150
14/14 [==============================] - 0s 566us/step - loss: 1829.2716
Epoch 144/150
14/14 [==============================] - 0s 554us/step - loss: 1828.0446
Epoch 145/150
14/14 [==============================] - 0s 554us/step - loss: 1827.2209
Epoch 146/150
14/14 [==============================] - 0s 551us/step - loss: 1828.5481
Epoch 147/150
14/14 [==============================] - 0s 566us/step - loss: 1839.5806
Epoch 148/150
14/14 [==============================] - 0s 565us/step - loss: 1831.0110
Epoch 149/150
14/14 [==============================] - 0s 579us/step - loss: 1820.8154
Epoch 150/150
14/14 [==============================] - 0s 590us/step - loss: 1820.2557
</pre></div>
</div>
</div>
</div>
<p>We get a printed output for each episodes, showing some information about the step. Below the ‘Epochs’, we see that a batch size of 30 at a 300 observations results in 10 (300 divided by 30) updates per epoch.</p>
<p>Of more interest is the rightmost piece of information, the loss. As the weights are adjusted, the model predictions fit the data progressively better - the loss decreases. At some point, the model in its current configuration reaches an optimal weight setting and the loss will oscillate around some value.</p>
<p>Note that by setting <code class="docutils literal notranslate"><span class="pre">verbose=0</span></code>, we can suppress any output.</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">history</span></code> we can now plot the loss over time/epochs. To do so, we use “our” <code class="docutils literal notranslate"><span class="pre">history</span></code> object and access its <code class="docutils literal notranslate"><span class="pre">.history</span></code> attribute, which is a dict:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;dict&#39;&gt;
dict_keys([&#39;loss&#39;])
</pre></div>
</div>
</div>
</div>
<p>Note that more values may be added to this dict. We will see this case for the next model.</p>
<p>With matplotlib, we plot the loss over time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x7faa18b00520&gt;]
</pre></div>
</div>
<img alt="_images/Keras_16_1.png" src="_images/Keras_16_1.png" />
</div>
</div>
<p>Apparently, the loss dropped very quickly in the beginning and reaches a minimal level after about 75 epochs.</p>
<p>Beside the <code class="docutils literal notranslate"><span class="pre">history</span></code> object, we can take a look at the trained weights by calling <code class="docutils literal notranslate"><span class="pre">model.get_weights()</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">: weights </span><span class="si">{</span><span class="n">layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hidden_layer: weights [array([[3.687329 , 2.6136518]], dtype=float32), array([-10.034517,  -7.102841], dtype=float32)]
output_layer: weights [array([[13.204637],
       [ 5.165712]], dtype=float32), array([33.05681], dtype=float32)]
</pre></div>
</div>
</div>
</div>
<p>For each layer, we see the number of entries corresponding to ‘params’ in the model summary above. The first array per layer gives the weights, the second array the bias.</p>
<p>Note here, that it is perfectly possible to save the weights of a model, e.g. in a file, and then load this model again later by setting these weights to the model weights. Of course, the model configuration must remain the same in order to use the weights. However, the already implemented <code class="docutils literal notranslate"><span class="pre">.save('my_path')</span></code> method does exactly that. To load the model, use <code class="docutils literal notranslate"><span class="pre">.model.load('my_path')</span></code>.</p>
<p>To see the predicitions from our model, we use the <code class="docutils literal notranslate"><span class="pre">.predict()</span></code> method, to which we only pass the x values, and store the result in a variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we can get a look on how this simple neural network approximates our data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Keras_22_0.png" src="_images/Keras_22_0.png" />
</div>
</div>
<p>Interestingly, the ReLU shape of the function can be seen rather clearly.</p>
<p>Now, let’s try to do better than that.</p>
<div class="section" id="a-more-complex-model">
<h2>A more complex model<a class="headerlink" href="#a-more-complex-model" title="Permalink to this headline">¶</a></h2>
<p>We will now test if a more flexible, more complex model with more layers and neurons, will approximate the data more accurately</p>
<p>First, we define a this new model as seen above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">random</span><span class="o">.</span><span class="n">set_seed</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>

<span class="n">better_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;l1&quot;</span><span class="p">),</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">11</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;l2&quot;</span><span class="p">),</span>
        <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;lo&quot;</span><span class="p">),</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>We will try a smaller learning rate (and may thus increase the number of epochs).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">better_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">6e-3</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">MeanSquaredError</span><span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Usually, when training a model, we split the data into a training and test set. We will do this using sklearn’s <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code>. Conveniently, in Keras, we can pass the validation data using the <code class="docutils literal notranslate"><span class="pre">validation_data</span></code> argument of <code class="docutils literal notranslate"><span class="pre">.fit()</span></code>.</p>
<p>Another possibility is to use <code class="docutils literal notranslate"><span class="pre">validation_split</span></code>, which will, when set for example to <code class="docutils literal notranslate"><span class="pre">.2</span></code>, omit 20% of the training data and evaluate the performance for this “unknown” part of the data. Beware, that this will only work for shuffled data, as the validation data is taken from the last entries of the data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span> <span class="mi">22</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">better_history</span> <span class="o">=</span> <span class="n">better_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/300
11/11 [==============================] - 0s 10ms/step - loss: 4423.1855 - val_loss: 4764.9458
Epoch 2/300
11/11 [==============================] - 0s 2ms/step - loss: 4059.0317 - val_loss: 4278.1099
Epoch 3/300
11/11 [==============================] - 0s 2ms/step - loss: 3626.5308 - val_loss: 3720.9053
Epoch 4/300
11/11 [==============================] - 0s 2ms/step - loss: 3185.1636 - val_loss: 3132.7334
Epoch 5/300
11/11 [==============================] - 0s 2ms/step - loss: 2836.7124 - val_loss: 2668.0583
Epoch 6/300
11/11 [==============================] - 0s 2ms/step - loss: 2599.5562 - val_loss: 2324.5557
Epoch 7/300
11/11 [==============================] - 0s 2ms/step - loss: 2489.2905 - val_loss: 2163.7839
Epoch 8/300
11/11 [==============================] - 0s 2ms/step - loss: 2437.2393 - val_loss: 2089.0007
Epoch 9/300
11/11 [==============================] - 0s 2ms/step - loss: 2427.4380 - val_loss: 2063.6453
Epoch 10/300
11/11 [==============================] - 0s 2ms/step - loss: 2407.3762 - val_loss: 2048.8547
Epoch 11/300
11/11 [==============================] - 0s 2ms/step - loss: 2407.6294 - val_loss: 2029.6438
Epoch 12/300
11/11 [==============================] - 0s 2ms/step - loss: 2382.4631 - val_loss: 1984.9429
Epoch 13/300
11/11 [==============================] - 0s 2ms/step - loss: 2384.2124 - val_loss: 1997.3094
Epoch 14/300
11/11 [==============================] - 0s 2ms/step - loss: 2367.7571 - val_loss: 1988.3342
Epoch 15/300
11/11 [==============================] - 0s 2ms/step - loss: 2362.3262 - val_loss: 1971.9707
Epoch 16/300
11/11 [==============================] - 0s 2ms/step - loss: 2352.3003 - val_loss: 1972.9386
Epoch 17/300
11/11 [==============================] - 0s 2ms/step - loss: 2347.5779 - val_loss: 1963.9500
Epoch 18/300
11/11 [==============================] - 0s 2ms/step - loss: 2330.2170 - val_loss: 1960.7595
Epoch 19/300
11/11 [==============================] - 0s 2ms/step - loss: 2321.6382 - val_loss: 1949.4347
Epoch 20/300
11/11 [==============================] - 0s 2ms/step - loss: 2320.4150 - val_loss: 1945.5450
Epoch 21/300
11/11 [==============================] - 0s 2ms/step - loss: 2307.5754 - val_loss: 1958.1935
Epoch 22/300
11/11 [==============================] - 0s 2ms/step - loss: 2299.6653 - val_loss: 1933.4589
Epoch 23/300
11/11 [==============================] - 0s 2ms/step - loss: 2289.7957 - val_loss: 1909.0164
Epoch 24/300
11/11 [==============================] - 0s 2ms/step - loss: 2280.6848 - val_loss: 1876.4146
Epoch 25/300
11/11 [==============================] - 0s 2ms/step - loss: 2276.5193 - val_loss: 1878.9860
Epoch 26/300
11/11 [==============================] - 0s 2ms/step - loss: 2254.3245 - val_loss: 1852.4825
Epoch 27/300
11/11 [==============================] - 0s 2ms/step - loss: 2250.8765 - val_loss: 1860.3118
Epoch 28/300
11/11 [==============================] - 0s 2ms/step - loss: 2238.4756 - val_loss: 1857.7333
Epoch 29/300
11/11 [==============================] - 0s 2ms/step - loss: 2223.3657 - val_loss: 1819.0338
Epoch 30/300
11/11 [==============================] - 0s 2ms/step - loss: 2215.9365 - val_loss: 1829.9100
Epoch 31/300
11/11 [==============================] - 0s 2ms/step - loss: 2205.3125 - val_loss: 1827.1034
Epoch 32/300
11/11 [==============================] - 0s 2ms/step - loss: 2191.2227 - val_loss: 1861.8767
Epoch 33/300
11/11 [==============================] - 0s 2ms/step - loss: 2184.6960 - val_loss: 1803.9115
Epoch 34/300
11/11 [==============================] - 0s 2ms/step - loss: 2170.4136 - val_loss: 1833.3708
Epoch 35/300
11/11 [==============================] - 0s 2ms/step - loss: 2156.1624 - val_loss: 1833.1395
Epoch 36/300
11/11 [==============================] - 0s 2ms/step - loss: 2143.2627 - val_loss: 1836.7909
Epoch 37/300
11/11 [==============================] - 0s 2ms/step - loss: 2139.5339 - val_loss: 1835.0956
Epoch 38/300
11/11 [==============================] - 0s 2ms/step - loss: 2126.5618 - val_loss: 1825.0632
Epoch 39/300
11/11 [==============================] - 0s 2ms/step - loss: 2109.2195 - val_loss: 1771.9906
Epoch 40/300
11/11 [==============================] - 0s 2ms/step - loss: 2086.7637 - val_loss: 1759.2365
Epoch 41/300
11/11 [==============================] - 0s 2ms/step - loss: 2076.6611 - val_loss: 1755.3175
Epoch 42/300
11/11 [==============================] - 0s 2ms/step - loss: 2070.6731 - val_loss: 1740.5669
Epoch 43/300
11/11 [==============================] - 0s 2ms/step - loss: 2040.1595 - val_loss: 1703.7441
Epoch 44/300
11/11 [==============================] - 0s 2ms/step - loss: 2035.1434 - val_loss: 1699.3903
Epoch 45/300
11/11 [==============================] - 0s 2ms/step - loss: 2012.0614 - val_loss: 1726.3231
Epoch 46/300
11/11 [==============================] - 0s 2ms/step - loss: 2003.1420 - val_loss: 1699.9490
Epoch 47/300
11/11 [==============================] - 0s 2ms/step - loss: 1987.6171 - val_loss: 1706.2931
Epoch 48/300
11/11 [==============================] - 0s 2ms/step - loss: 1973.0715 - val_loss: 1710.7201
Epoch 49/300
11/11 [==============================] - 0s 2ms/step - loss: 1955.0940 - val_loss: 1659.9407
Epoch 50/300
11/11 [==============================] - 0s 2ms/step - loss: 1938.7159 - val_loss: 1668.3157
Epoch 51/300
11/11 [==============================] - 0s 2ms/step - loss: 1929.2072 - val_loss: 1646.7941
Epoch 52/300
11/11 [==============================] - 0s 2ms/step - loss: 1905.4500 - val_loss: 1619.4000
Epoch 53/300
11/11 [==============================] - 0s 2ms/step - loss: 1909.9764 - val_loss: 1623.9551
Epoch 54/300
11/11 [==============================] - 0s 2ms/step - loss: 1879.2267 - val_loss: 1636.4708
Epoch 55/300
11/11 [==============================] - 0s 2ms/step - loss: 1868.8823 - val_loss: 1621.0387
Epoch 56/300
11/11 [==============================] - 0s 2ms/step - loss: 1854.0105 - val_loss: 1613.7264
Epoch 57/300
11/11 [==============================] - 0s 2ms/step - loss: 1846.2897 - val_loss: 1583.8146
Epoch 58/300
11/11 [==============================] - 0s 2ms/step - loss: 1831.8914 - val_loss: 1598.0284
Epoch 59/300
11/11 [==============================] - 0s 2ms/step - loss: 1817.3688 - val_loss: 1568.4608
Epoch 60/300
11/11 [==============================] - 0s 2ms/step - loss: 1801.6073 - val_loss: 1560.3635
Epoch 61/300
11/11 [==============================] - 0s 2ms/step - loss: 1786.0494 - val_loss: 1600.6619
Epoch 62/300
11/11 [==============================] - 0s 2ms/step - loss: 1780.4113 - val_loss: 1557.3508
Epoch 63/300
11/11 [==============================] - 0s 2ms/step - loss: 1772.9723 - val_loss: 1536.3665
Epoch 64/300
11/11 [==============================] - 0s 2ms/step - loss: 1776.1638 - val_loss: 1542.3396
Epoch 65/300
11/11 [==============================] - 0s 2ms/step - loss: 1759.9691 - val_loss: 1531.1531
Epoch 66/300
11/11 [==============================] - 0s 2ms/step - loss: 1747.2798 - val_loss: 1501.9725
Epoch 67/300
11/11 [==============================] - 0s 2ms/step - loss: 1738.1222 - val_loss: 1551.6027
Epoch 68/300
11/11 [==============================] - 0s 2ms/step - loss: 1739.1207 - val_loss: 1524.2706
Epoch 69/300
11/11 [==============================] - 0s 2ms/step - loss: 1716.2754 - val_loss: 1564.8651
Epoch 70/300
11/11 [==============================] - 0s 2ms/step - loss: 1722.5901 - val_loss: 1514.6958
Epoch 71/300
11/11 [==============================] - 0s 2ms/step - loss: 1710.8722 - val_loss: 1515.2950
Epoch 72/300
11/11 [==============================] - 0s 2ms/step - loss: 1712.9485 - val_loss: 1556.0962
Epoch 73/300
11/11 [==============================] - 0s 2ms/step - loss: 1707.2096 - val_loss: 1521.9677
Epoch 74/300
11/11 [==============================] - 0s 2ms/step - loss: 1707.4171 - val_loss: 1517.0468
Epoch 75/300
11/11 [==============================] - 0s 2ms/step - loss: 1689.7064 - val_loss: 1554.9518
Epoch 76/300
11/11 [==============================] - 0s 2ms/step - loss: 1697.2592 - val_loss: 1548.3118
Epoch 77/300
11/11 [==============================] - 0s 2ms/step - loss: 1695.2754 - val_loss: 1520.6616
Epoch 78/300
11/11 [==============================] - 0s 2ms/step - loss: 1682.7366 - val_loss: 1532.7892
Epoch 79/300
11/11 [==============================] - 0s 2ms/step - loss: 1684.2969 - val_loss: 1524.4120
Epoch 80/300
11/11 [==============================] - 0s 2ms/step - loss: 1691.0632 - val_loss: 1528.9172
Epoch 81/300
11/11 [==============================] - 0s 2ms/step - loss: 1678.7395 - val_loss: 1515.5262
Epoch 82/300
11/11 [==============================] - 0s 2ms/step - loss: 1674.1667 - val_loss: 1553.6654
Epoch 83/300
11/11 [==============================] - 0s 2ms/step - loss: 1660.8284 - val_loss: 1498.9338
Epoch 84/300
11/11 [==============================] - 0s 2ms/step - loss: 1669.5421 - val_loss: 1517.6790
Epoch 85/300
11/11 [==============================] - 0s 2ms/step - loss: 1668.9930 - val_loss: 1519.8484
Epoch 86/300
11/11 [==============================] - 0s 2ms/step - loss: 1659.4877 - val_loss: 1564.2252
Epoch 87/300
11/11 [==============================] - 0s 2ms/step - loss: 1654.0789 - val_loss: 1504.6898
Epoch 88/300
11/11 [==============================] - 0s 2ms/step - loss: 1662.3572 - val_loss: 1499.4330
Epoch 89/300
11/11 [==============================] - 0s 2ms/step - loss: 1652.7659 - val_loss: 1537.8497
Epoch 90/300
11/11 [==============================] - 0s 2ms/step - loss: 1648.4963 - val_loss: 1552.9883
Epoch 91/300
11/11 [==============================] - 0s 2ms/step - loss: 1646.8544 - val_loss: 1547.1010
Epoch 92/300
11/11 [==============================] - 0s 2ms/step - loss: 1640.8298 - val_loss: 1524.5757
Epoch 93/300
11/11 [==============================] - 0s 2ms/step - loss: 1655.3790 - val_loss: 1534.5071
Epoch 94/300
11/11 [==============================] - 0s 2ms/step - loss: 1636.5000 - val_loss: 1572.8560
Epoch 95/300
11/11 [==============================] - 0s 2ms/step - loss: 1642.4287 - val_loss: 1526.5636
Epoch 96/300
11/11 [==============================] - 0s 2ms/step - loss: 1636.4489 - val_loss: 1510.4445
Epoch 97/300
11/11 [==============================] - 0s 2ms/step - loss: 1629.0291 - val_loss: 1511.6960
Epoch 98/300
11/11 [==============================] - 0s 2ms/step - loss: 1630.6163 - val_loss: 1538.9628
Epoch 99/300
11/11 [==============================] - 0s 2ms/step - loss: 1633.2094 - val_loss: 1525.2015
Epoch 100/300
11/11 [==============================] - 0s 2ms/step - loss: 1626.9652 - val_loss: 1543.1016
Epoch 101/300
11/11 [==============================] - 0s 2ms/step - loss: 1627.0695 - val_loss: 1564.2291
Epoch 102/300
11/11 [==============================] - 0s 2ms/step - loss: 1621.2656 - val_loss: 1527.1366
Epoch 103/300
11/11 [==============================] - 0s 2ms/step - loss: 1638.5375 - val_loss: 1561.5431
Epoch 104/300
11/11 [==============================] - 0s 2ms/step - loss: 1635.5902 - val_loss: 1559.9377
Epoch 105/300
11/11 [==============================] - 0s 2ms/step - loss: 1631.9353 - val_loss: 1526.4905
Epoch 106/300
11/11 [==============================] - 0s 2ms/step - loss: 1631.1542 - val_loss: 1565.6115
Epoch 107/300
11/11 [==============================] - 0s 2ms/step - loss: 1621.7507 - val_loss: 1556.8276
Epoch 108/300
11/11 [==============================] - 0s 2ms/step - loss: 1609.6445 - val_loss: 1537.3052
Epoch 109/300
11/11 [==============================] - 0s 2ms/step - loss: 1622.6921 - val_loss: 1569.3217
Epoch 110/300
11/11 [==============================] - 0s 2ms/step - loss: 1625.3201 - val_loss: 1535.8531
Epoch 111/300
11/11 [==============================] - 0s 2ms/step - loss: 1609.6055 - val_loss: 1558.5278
Epoch 112/300
11/11 [==============================] - 0s 2ms/step - loss: 1608.5817 - val_loss: 1626.5984
Epoch 113/300
11/11 [==============================] - 0s 2ms/step - loss: 1617.4250 - val_loss: 1555.0901
Epoch 114/300
11/11 [==============================] - 0s 2ms/step - loss: 1608.8340 - val_loss: 1548.9974
Epoch 115/300
11/11 [==============================] - 0s 2ms/step - loss: 1606.7340 - val_loss: 1509.8851
Epoch 116/300
11/11 [==============================] - 0s 2ms/step - loss: 1608.7386 - val_loss: 1540.9830
Epoch 117/300
11/11 [==============================] - 0s 2ms/step - loss: 1634.4485 - val_loss: 1551.6420
Epoch 118/300
11/11 [==============================] - 0s 2ms/step - loss: 1603.2123 - val_loss: 1567.3442
Epoch 119/300
11/11 [==============================] - 0s 2ms/step - loss: 1601.6088 - val_loss: 1602.0024
Epoch 120/300
11/11 [==============================] - 0s 2ms/step - loss: 1601.8127 - val_loss: 1543.4882
Epoch 121/300
11/11 [==============================] - 0s 2ms/step - loss: 1601.4474 - val_loss: 1543.1669
Epoch 122/300
11/11 [==============================] - 0s 2ms/step - loss: 1599.8573 - val_loss: 1519.5914
Epoch 123/300
11/11 [==============================] - 0s 2ms/step - loss: 1603.9243 - val_loss: 1552.5315
Epoch 124/300
11/11 [==============================] - 0s 2ms/step - loss: 1593.6224 - val_loss: 1530.9850
Epoch 125/300
11/11 [==============================] - 0s 2ms/step - loss: 1601.7449 - val_loss: 1556.5278
Epoch 126/300
11/11 [==============================] - 0s 2ms/step - loss: 1604.2119 - val_loss: 1577.8442
Epoch 127/300
11/11 [==============================] - 0s 2ms/step - loss: 1597.8873 - val_loss: 1545.0284
Epoch 128/300
11/11 [==============================] - 0s 2ms/step - loss: 1601.1166 - val_loss: 1584.9552
Epoch 129/300
11/11 [==============================] - 0s 2ms/step - loss: 1597.3550 - val_loss: 1583.9049
Epoch 130/300
11/11 [==============================] - 0s 2ms/step - loss: 1598.3264 - val_loss: 1572.8289
Epoch 131/300
11/11 [==============================] - 0s 2ms/step - loss: 1593.5022 - val_loss: 1557.4766
Epoch 132/300
11/11 [==============================] - 0s 2ms/step - loss: 1592.7837 - val_loss: 1543.2180
Epoch 133/300
11/11 [==============================] - 0s 2ms/step - loss: 1603.6610 - val_loss: 1554.4800
Epoch 134/300
11/11 [==============================] - 0s 2ms/step - loss: 1589.2123 - val_loss: 1536.2039
Epoch 135/300
11/11 [==============================] - 0s 2ms/step - loss: 1594.7056 - val_loss: 1561.5310
Epoch 136/300
11/11 [==============================] - 0s 2ms/step - loss: 1595.2435 - val_loss: 1557.5048
Epoch 137/300
11/11 [==============================] - 0s 2ms/step - loss: 1591.3816 - val_loss: 1546.0234
Epoch 138/300
11/11 [==============================] - 0s 2ms/step - loss: 1592.2623 - val_loss: 1548.6661
Epoch 139/300
11/11 [==============================] - 0s 2ms/step - loss: 1600.5662 - val_loss: 1547.2429
Epoch 140/300
11/11 [==============================] - 0s 2ms/step - loss: 1599.4938 - val_loss: 1562.3004
Epoch 141/300
11/11 [==============================] - 0s 2ms/step - loss: 1571.6519 - val_loss: 1525.0636
Epoch 142/300
11/11 [==============================] - 0s 2ms/step - loss: 1591.9324 - val_loss: 1573.1453
Epoch 143/300
11/11 [==============================] - 0s 2ms/step - loss: 1585.0814 - val_loss: 1572.6677
Epoch 144/300
11/11 [==============================] - 0s 2ms/step - loss: 1591.8785 - val_loss: 1554.7677
Epoch 145/300
11/11 [==============================] - 0s 2ms/step - loss: 1575.1523 - val_loss: 1587.6812
Epoch 146/300
11/11 [==============================] - 0s 2ms/step - loss: 1574.8285 - val_loss: 1568.2628
Epoch 147/300
11/11 [==============================] - 0s 2ms/step - loss: 1594.5159 - val_loss: 1570.6367
Epoch 148/300
11/11 [==============================] - 0s 2ms/step - loss: 1584.5305 - val_loss: 1546.7961
Epoch 149/300
11/11 [==============================] - 0s 2ms/step - loss: 1581.7692 - val_loss: 1532.8899
Epoch 150/300
11/11 [==============================] - 0s 2ms/step - loss: 1597.2368 - val_loss: 1539.4625
Epoch 151/300
11/11 [==============================] - 0s 2ms/step - loss: 1582.0146 - val_loss: 1520.5320
Epoch 152/300
11/11 [==============================] - 0s 2ms/step - loss: 1600.4520 - val_loss: 1517.5708
Epoch 153/300
11/11 [==============================] - 0s 2ms/step - loss: 1582.8906 - val_loss: 1525.6820
Epoch 154/300
11/11 [==============================] - 0s 2ms/step - loss: 1585.0962 - val_loss: 1554.5916
Epoch 155/300
11/11 [==============================] - 0s 2ms/step - loss: 1584.7213 - val_loss: 1552.6920
Epoch 156/300
11/11 [==============================] - 0s 2ms/step - loss: 1583.2677 - val_loss: 1552.2941
Epoch 157/300
11/11 [==============================] - 0s 2ms/step - loss: 1604.7129 - val_loss: 1577.1658
Epoch 158/300
11/11 [==============================] - 0s 2ms/step - loss: 1579.1442 - val_loss: 1541.2098
Epoch 159/300
11/11 [==============================] - 0s 2ms/step - loss: 1590.1934 - val_loss: 1562.9781
Epoch 160/300
11/11 [==============================] - 0s 2ms/step - loss: 1581.8678 - val_loss: 1525.3079
Epoch 161/300
11/11 [==============================] - 0s 2ms/step - loss: 1582.3097 - val_loss: 1546.0269
Epoch 162/300
11/11 [==============================] - 0s 2ms/step - loss: 1587.5035 - val_loss: 1544.0813
Epoch 163/300
11/11 [==============================] - 0s 2ms/step - loss: 1584.4094 - val_loss: 1544.0259
Epoch 164/300
11/11 [==============================] - 0s 2ms/step - loss: 1579.3372 - val_loss: 1567.4923
Epoch 165/300
11/11 [==============================] - 0s 2ms/step - loss: 1584.9952 - val_loss: 1557.6849
Epoch 166/300
11/11 [==============================] - 0s 2ms/step - loss: 1575.2402 - val_loss: 1605.4515
Epoch 167/300
11/11 [==============================] - 0s 2ms/step - loss: 1590.4186 - val_loss: 1547.1206
Epoch 168/300
11/11 [==============================] - 0s 2ms/step - loss: 1584.8517 - val_loss: 1534.5641
Epoch 169/300
11/11 [==============================] - 0s 2ms/step - loss: 1573.8840 - val_loss: 1586.3196
Epoch 170/300
11/11 [==============================] - 0s 2ms/step - loss: 1567.0675 - val_loss: 1563.1681
Epoch 171/300
11/11 [==============================] - 0s 2ms/step - loss: 1577.1860 - val_loss: 1536.2443
Epoch 172/300
11/11 [==============================] - 0s 2ms/step - loss: 1586.8375 - val_loss: 1521.8265
Epoch 173/300
11/11 [==============================] - 0s 2ms/step - loss: 1570.8157 - val_loss: 1623.8457
Epoch 174/300
11/11 [==============================] - 0s 2ms/step - loss: 1593.0073 - val_loss: 1567.9115
Epoch 175/300
11/11 [==============================] - 0s 2ms/step - loss: 1574.0054 - val_loss: 1570.0867
Epoch 176/300
11/11 [==============================] - 0s 2ms/step - loss: 1576.5784 - val_loss: 1579.1342
Epoch 177/300
11/11 [==============================] - 0s 2ms/step - loss: 1566.2333 - val_loss: 1573.2433
Epoch 178/300
11/11 [==============================] - 0s 2ms/step - loss: 1589.8158 - val_loss: 1578.4375
Epoch 179/300
11/11 [==============================] - 0s 2ms/step - loss: 1595.2684 - val_loss: 1551.7893
Epoch 180/300
11/11 [==============================] - 0s 2ms/step - loss: 1575.2947 - val_loss: 1570.6444
Epoch 181/300
11/11 [==============================] - 0s 2ms/step - loss: 1575.6421 - val_loss: 1553.1021
Epoch 182/300
11/11 [==============================] - 0s 2ms/step - loss: 1589.1985 - val_loss: 1530.7257
Epoch 183/300
11/11 [==============================] - 0s 2ms/step - loss: 1574.0476 - val_loss: 1584.0522
Epoch 184/300
11/11 [==============================] - 0s 2ms/step - loss: 1579.0439 - val_loss: 1561.8593
Epoch 185/300
11/11 [==============================] - 0s 2ms/step - loss: 1585.8994 - val_loss: 1592.4614
Epoch 186/300
11/11 [==============================] - 0s 2ms/step - loss: 1580.5999 - val_loss: 1586.7515
Epoch 187/300
11/11 [==============================] - 0s 2ms/step - loss: 1575.4419 - val_loss: 1583.8198
Epoch 188/300
11/11 [==============================] - 0s 2ms/step - loss: 1581.0305 - val_loss: 1544.8184
Epoch 189/300
11/11 [==============================] - 0s 2ms/step - loss: 1582.2794 - val_loss: 1558.7670
Epoch 190/300
11/11 [==============================] - 0s 2ms/step - loss: 1555.6042 - val_loss: 1623.3396
Epoch 191/300
11/11 [==============================] - 0s 2ms/step - loss: 1576.8892 - val_loss: 1534.8136
Epoch 192/300
11/11 [==============================] - 0s 2ms/step - loss: 1582.4779 - val_loss: 1562.0891
Epoch 193/300
11/11 [==============================] - 0s 2ms/step - loss: 1588.1860 - val_loss: 1546.0768
Epoch 194/300
11/11 [==============================] - 0s 2ms/step - loss: 1566.7875 - val_loss: 1536.5773
Epoch 195/300
11/11 [==============================] - 0s 2ms/step - loss: 1570.4266 - val_loss: 1548.6268
Epoch 196/300
11/11 [==============================] - 0s 2ms/step - loss: 1568.4626 - val_loss: 1570.4242
Epoch 197/300
11/11 [==============================] - 0s 2ms/step - loss: 1594.3583 - val_loss: 1552.9857
Epoch 198/300
11/11 [==============================] - 0s 2ms/step - loss: 1571.0608 - val_loss: 1570.5228
Epoch 199/300
11/11 [==============================] - 0s 2ms/step - loss: 1573.9783 - val_loss: 1607.8411
Epoch 200/300
11/11 [==============================] - 0s 2ms/step - loss: 1567.0817 - val_loss: 1536.7209
Epoch 201/300
11/11 [==============================] - 0s 2ms/step - loss: 1582.2728 - val_loss: 1554.9406
Epoch 202/300
11/11 [==============================] - 0s 2ms/step - loss: 1577.5270 - val_loss: 1580.7789
Epoch 203/300
11/11 [==============================] - 0s 2ms/step - loss: 1569.0411 - val_loss: 1597.0370
Epoch 204/300
11/11 [==============================] - 0s 2ms/step - loss: 1565.9851 - val_loss: 1631.4062
Epoch 205/300
11/11 [==============================] - 0s 2ms/step - loss: 1567.3202 - val_loss: 1551.7507
Epoch 206/300
11/11 [==============================] - 0s 2ms/step - loss: 1585.5850 - val_loss: 1598.4767
Epoch 207/300
11/11 [==============================] - 0s 2ms/step - loss: 1566.3518 - val_loss: 1619.4183
Epoch 208/300
11/11 [==============================] - 0s 2ms/step - loss: 1574.1832 - val_loss: 1560.6444
Epoch 209/300
11/11 [==============================] - 0s 2ms/step - loss: 1580.4655 - val_loss: 1560.6724
Epoch 210/300
11/11 [==============================] - 0s 2ms/step - loss: 1572.0847 - val_loss: 1565.8807
Epoch 211/300
11/11 [==============================] - 0s 2ms/step - loss: 1570.1130 - val_loss: 1580.2874
Epoch 212/300
11/11 [==============================] - 0s 2ms/step - loss: 1576.7911 - val_loss: 1550.1537
Epoch 213/300
11/11 [==============================] - 0s 2ms/step - loss: 1564.2692 - val_loss: 1594.7606
Epoch 214/300
11/11 [==============================] - 0s 2ms/step - loss: 1572.5992 - val_loss: 1572.5922
Epoch 215/300
11/11 [==============================] - 0s 2ms/step - loss: 1569.0338 - val_loss: 1551.5193
Epoch 216/300
11/11 [==============================] - 0s 2ms/step - loss: 1569.7334 - val_loss: 1595.5281
Epoch 217/300
11/11 [==============================] - 0s 2ms/step - loss: 1571.8217 - val_loss: 1612.4626
Epoch 218/300
11/11 [==============================] - 0s 2ms/step - loss: 1573.0623 - val_loss: 1559.8917
Epoch 219/300
11/11 [==============================] - 0s 2ms/step - loss: 1573.8481 - val_loss: 1575.1794
Epoch 220/300
11/11 [==============================] - 0s 2ms/step - loss: 1555.1716 - val_loss: 1618.5461
Epoch 221/300
11/11 [==============================] - 0s 2ms/step - loss: 1580.7625 - val_loss: 1562.4581
Epoch 222/300
11/11 [==============================] - 0s 2ms/step - loss: 1554.1061 - val_loss: 1579.8020
Epoch 223/300
11/11 [==============================] - 0s 2ms/step - loss: 1578.8394 - val_loss: 1579.0542
Epoch 224/300
11/11 [==============================] - 0s 2ms/step - loss: 1565.7498 - val_loss: 1538.3357
Epoch 225/300
11/11 [==============================] - 0s 2ms/step - loss: 1560.6449 - val_loss: 1584.1310
Epoch 226/300
11/11 [==============================] - 0s 2ms/step - loss: 1586.7312 - val_loss: 1576.9091
Epoch 227/300
11/11 [==============================] - 0s 2ms/step - loss: 1569.4865 - val_loss: 1547.1921
Epoch 228/300
11/11 [==============================] - 0s 2ms/step - loss: 1574.4011 - val_loss: 1557.0574
Epoch 229/300
11/11 [==============================] - 0s 2ms/step - loss: 1569.9548 - val_loss: 1571.8396
Epoch 230/300
11/11 [==============================] - 0s 2ms/step - loss: 1573.1643 - val_loss: 1558.2521
Epoch 231/300
11/11 [==============================] - 0s 2ms/step - loss: 1578.2638 - val_loss: 1605.5591
Epoch 232/300
11/11 [==============================] - 0s 2ms/step - loss: 1571.3629 - val_loss: 1562.9531
Epoch 233/300
11/11 [==============================] - 0s 2ms/step - loss: 1549.4421 - val_loss: 1595.6134
Epoch 234/300
11/11 [==============================] - 0s 2ms/step - loss: 1577.4187 - val_loss: 1565.2087
Epoch 235/300
11/11 [==============================] - 0s 2ms/step - loss: 1572.1504 - val_loss: 1597.2532
Epoch 236/300
11/11 [==============================] - 0s 2ms/step - loss: 1560.6211 - val_loss: 1604.7072
Epoch 237/300
11/11 [==============================] - 0s 2ms/step - loss: 1575.2947 - val_loss: 1543.2906
Epoch 238/300
11/11 [==============================] - 0s 2ms/step - loss: 1544.9934 - val_loss: 1626.3749
Epoch 239/300
11/11 [==============================] - 0s 2ms/step - loss: 1570.3925 - val_loss: 1556.6169
Epoch 240/300
11/11 [==============================] - 0s 2ms/step - loss: 1572.1591 - val_loss: 1582.3872
Epoch 241/300
11/11 [==============================] - 0s 2ms/step - loss: 1564.7419 - val_loss: 1584.8131
Epoch 242/300
11/11 [==============================] - 0s 2ms/step - loss: 1557.1899 - val_loss: 1557.7401
Epoch 243/300
11/11 [==============================] - 0s 2ms/step - loss: 1570.7762 - val_loss: 1587.8607
Epoch 244/300
11/11 [==============================] - 0s 2ms/step - loss: 1553.3586 - val_loss: 1594.0687
Epoch 245/300
11/11 [==============================] - 0s 2ms/step - loss: 1564.3462 - val_loss: 1554.9625
Epoch 246/300
11/11 [==============================] - 0s 2ms/step - loss: 1558.7959 - val_loss: 1576.0776
Epoch 247/300
11/11 [==============================] - 0s 2ms/step - loss: 1577.1257 - val_loss: 1580.2261
Epoch 248/300
11/11 [==============================] - 0s 2ms/step - loss: 1557.5142 - val_loss: 1538.2916
Epoch 249/300
11/11 [==============================] - 0s 2ms/step - loss: 1579.5326 - val_loss: 1556.5726
Epoch 250/300
11/11 [==============================] - 0s 2ms/step - loss: 1570.4338 - val_loss: 1602.2930
Epoch 251/300
11/11 [==============================] - 0s 2ms/step - loss: 1552.5555 - val_loss: 1582.3070
Epoch 252/300
11/11 [==============================] - 0s 2ms/step - loss: 1562.6261 - val_loss: 1543.5740
Epoch 253/300
11/11 [==============================] - 0s 2ms/step - loss: 1568.2202 - val_loss: 1596.4591
Epoch 254/300
11/11 [==============================] - 0s 2ms/step - loss: 1567.4917 - val_loss: 1603.4777
Epoch 255/300
11/11 [==============================] - 0s 2ms/step - loss: 1564.7239 - val_loss: 1535.4740
Epoch 256/300
11/11 [==============================] - 0s 2ms/step - loss: 1567.7380 - val_loss: 1546.9358
Epoch 257/300
11/11 [==============================] - 0s 2ms/step - loss: 1559.7917 - val_loss: 1540.4062
Epoch 258/300
11/11 [==============================] - 0s 2ms/step - loss: 1563.7710 - val_loss: 1569.5797
Epoch 259/300
11/11 [==============================] - 0s 2ms/step - loss: 1562.3267 - val_loss: 1570.1714
Epoch 260/300
11/11 [==============================] - 0s 2ms/step - loss: 1567.4618 - val_loss: 1529.7367
Epoch 261/300
11/11 [==============================] - 0s 2ms/step - loss: 1560.2017 - val_loss: 1541.1654
Epoch 262/300
11/11 [==============================] - 0s 2ms/step - loss: 1571.0651 - val_loss: 1550.7667
Epoch 263/300
11/11 [==============================] - 0s 2ms/step - loss: 1549.1348 - val_loss: 1552.2969
Epoch 264/300
11/11 [==============================] - 0s 2ms/step - loss: 1554.8848 - val_loss: 1581.8367
Epoch 265/300
11/11 [==============================] - 0s 2ms/step - loss: 1567.4281 - val_loss: 1592.4705
Epoch 266/300
11/11 [==============================] - 0s 2ms/step - loss: 1567.9866 - val_loss: 1579.9174
Epoch 267/300
11/11 [==============================] - 0s 2ms/step - loss: 1561.2045 - val_loss: 1555.7815
Epoch 268/300
11/11 [==============================] - 0s 2ms/step - loss: 1556.8666 - val_loss: 1592.7440
Epoch 269/300
11/11 [==============================] - 0s 2ms/step - loss: 1564.5590 - val_loss: 1585.8793
Epoch 270/300
11/11 [==============================] - 0s 2ms/step - loss: 1546.4834 - val_loss: 1536.9432
Epoch 271/300
11/11 [==============================] - 0s 2ms/step - loss: 1559.5850 - val_loss: 1605.3159
Epoch 272/300
11/11 [==============================] - 0s 2ms/step - loss: 1560.4030 - val_loss: 1587.2866
Epoch 273/300
11/11 [==============================] - 0s 2ms/step - loss: 1558.9346 - val_loss: 1641.6567
Epoch 274/300
11/11 [==============================] - 0s 2ms/step - loss: 1558.9396 - val_loss: 1603.2734
Epoch 275/300
11/11 [==============================] - 0s 2ms/step - loss: 1550.6918 - val_loss: 1543.0065
Epoch 276/300
11/11 [==============================] - 0s 2ms/step - loss: 1540.8167 - val_loss: 1611.1958
Epoch 277/300
11/11 [==============================] - 0s 2ms/step - loss: 1560.6385 - val_loss: 1560.2147
Epoch 278/300
11/11 [==============================] - 0s 2ms/step - loss: 1563.7156 - val_loss: 1556.2844
Epoch 279/300
11/11 [==============================] - 0s 2ms/step - loss: 1549.4388 - val_loss: 1585.1172
Epoch 280/300
11/11 [==============================] - 0s 2ms/step - loss: 1554.1494 - val_loss: 1559.1740
Epoch 281/300
11/11 [==============================] - 0s 2ms/step - loss: 1557.2256 - val_loss: 1586.3242
Epoch 282/300
11/11 [==============================] - 0s 2ms/step - loss: 1574.8818 - val_loss: 1595.7247
Epoch 283/300
11/11 [==============================] - 0s 2ms/step - loss: 1557.7780 - val_loss: 1551.9180
Epoch 284/300
11/11 [==============================] - 0s 2ms/step - loss: 1555.5768 - val_loss: 1610.3619
Epoch 285/300
11/11 [==============================] - 0s 2ms/step - loss: 1551.4695 - val_loss: 1550.2325
Epoch 286/300
11/11 [==============================] - 0s 2ms/step - loss: 1558.0886 - val_loss: 1637.1415
Epoch 287/300
11/11 [==============================] - 0s 2ms/step - loss: 1565.4258 - val_loss: 1564.7794
Epoch 288/300
11/11 [==============================] - 0s 2ms/step - loss: 1556.3398 - val_loss: 1561.7880
Epoch 289/300
11/11 [==============================] - 0s 2ms/step - loss: 1557.1294 - val_loss: 1528.2310
Epoch 290/300
11/11 [==============================] - 0s 2ms/step - loss: 1570.4331 - val_loss: 1584.6252
Epoch 291/300
11/11 [==============================] - 0s 2ms/step - loss: 1550.8514 - val_loss: 1601.6328
Epoch 292/300
11/11 [==============================] - 0s 2ms/step - loss: 1546.9905 - val_loss: 1604.6683
Epoch 293/300
11/11 [==============================] - 0s 2ms/step - loss: 1549.1362 - val_loss: 1606.8207
Epoch 294/300
11/11 [==============================] - 0s 2ms/step - loss: 1550.2656 - val_loss: 1550.4507
Epoch 295/300
11/11 [==============================] - 0s 2ms/step - loss: 1540.8870 - val_loss: 1582.9281
Epoch 296/300
11/11 [==============================] - 0s 2ms/step - loss: 1570.1682 - val_loss: 1558.8130
Epoch 297/300
11/11 [==============================] - 0s 2ms/step - loss: 1545.7144 - val_loss: 1529.3497
Epoch 298/300
11/11 [==============================] - 0s 2ms/step - loss: 1559.2699 - val_loss: 1546.9230
Epoch 299/300
11/11 [==============================] - 0s 2ms/step - loss: 1558.9611 - val_loss: 1590.2181
Epoch 300/300
11/11 [==============================] - 0s 2ms/step - loss: 1560.4011 - val_loss: 1547.3069
</pre></div>
</div>
</div>
</div>
<p>We can see the additional information given for the loss when applying the current weights of each epoch to the
validation data.
Let’s have a look at it.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">better_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;training&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">better_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;val_loss&#39;</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;validation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Keras_33_0.png" src="_images/Keras_33_0.png" />
</div>
</div>
<p>We see a moderate, yet steady decrease in both curves before hitting a plateau of slight oscillation. The graphic shows, that we do not have to deal with overfitting and that our model seems to have converged.</p>
<p>Finally, let’s have a look at the predictions of the trained model again:</p>
<p>There are ways to handle overfitting but we will not discuss them here.</p>
<p>Finally, let’s have a look at the trained model again, using all the data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">better_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">make_data</span><span class="p">(</span><span class="o">-</span><span class="mf">.5</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mf">1.5</span><span class="p">,</span><span class="n">noise</span><span class="o">=</span><span class="kc">False</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y_pred</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;orange&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/Keras_36_0.png" src="_images/Keras_36_0.png" />
</div>
</div>
<p>At last, we see that our more complex model is very well able to find the underlying function which we used to generate our data.</p>
<p>While we have only covered a tiny bit of TensorFlow and Keras, the syntax stays the same for different kinds of networks. For example, to build a convolutional artificial neural network, we would use <code class="docutils literal notranslate"><span class="pre">keras.Conv2D()</span></code> instead of <code class="docutils literal notranslate"><span class="pre">keras.Dense</span></code>. For more types of layers and how to build more complex architectures, visit the documentation refered to above.</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Jan König<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>